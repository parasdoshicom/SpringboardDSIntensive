{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Science Project Steps\n",
    "\n",
    "1. Import data\n",
    "\n",
    "2. Exploratory data analysis\n",
    "\n",
    "3. Data wrangling\n",
    "\n",
    "4. Model Training\n",
    "\n",
    "5. Model Testing\n",
    "\n",
    "6. Inferencing\n",
    "\n",
    "7. Data storytelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(filename) # From a CSV file\n",
    "pd.read_table(filename) # From a delimited text file (like TSV)\n",
    "pd.read_excel(filename) # From an Excel file\n",
    "pd.read_sql(query, connection_object) # Reads from a SQL table/database\n",
    "pd.read_json(json_string) # Reads from a JSON formatted string, URL or file.\n",
    "pd.read_html(url) # Parses an html URL, string or file and extracts tables to a list of dataframes\n",
    "pd.read_clipboard() # Takes the contents of your clipboard and passes it to read_table()\n",
    "pd.DataFrame(dict) # From a dict, keys for columns names, values for data as lists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Exploratory data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploratory Data\n",
    "\n",
    "df.shape() # Prints number of rows and columns in dataframe\n",
    "df.head(n) # Prints first n rows of the DataFrame\n",
    "df.tail(n) # Prints last n rows of the DataFrame\n",
    "df.info() # Index, Datatype and Memory information\n",
    "df.describe() # Summary statistics for numerical columns\n",
    "s.value_counts(dropna=False) # Views unique values and counts\n",
    "df.apply(pd.Series.value_counts) # Unique values and counts for all columns\n",
    "df.describe() # Summary statistics for numerical columns\n",
    "df.mean() # Returns the mean of all columns\n",
    "df.corr() # Returns the correlation between columns in a DataFrame\n",
    "df.count() # Returns the number of non-null values in each DataFrame column\n",
    "df.max() # Returns the highest value in each column\n",
    "df.min() # Returns the lowest value in each column\n",
    "df.median() # Returns the median of each column\n",
    "df.std() # Returns the standard deviation of each column\n",
    "\n",
    "# Selecting data\n",
    "\n",
    "df[col] # Returns column with label col as Series\n",
    "df[[col1, col2]] # Returns Columns as a new DataFrame\n",
    "s.iloc[0] # Selection by position (selects first element)\n",
    "s.loc[0] # Selection by index (selects element at index 0)\n",
    "df.iloc[0,:] # First row\n",
    "df.iloc[0,0] # First element of first column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Data wrangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Cleaning\n",
    "\n",
    "df.columns = ['a','b','c'] # Renames columns\n",
    "pd.isnull() # Checks for null Values, Returns Boolean Array\n",
    "pd.notnull() # Opposite of s.isnull()\n",
    "df.dropna() # Drops all rows that contain null values\n",
    "df.dropna(axis=1) # Drops all columns that contain null values\n",
    "df.dropna(axis=1,thresh=n) # Drops all rows have have less than n non null values\n",
    "df.fillna(x) # Replaces all null values with x\n",
    "s.fillna(s.mean()) # Replaces all null values with the mean (mean can be replaced with almost any function from the statistics section)\n",
    "s.astype(float) # Converts the datatype of the series to float\n",
    "s.replace(1,'one') # Replaces all values equal to 1 with 'one'\n",
    "s.replace([1,3],['one','three']) # Replaces all 1 with 'one' and 3 with 'three'\n",
    "df.rename(columns=lambda x: x + 1) # Mass renaming of columns\n",
    "df.rename(columns={'old_name': 'new_ name'}) # Selective renaming\n",
    "df.set_index('column_one') # Changes the index\n",
    "df.rename(index=lambda x: x + 1) # Mass renaming of index\n",
    "\n",
    "#Filter, Sort and Group By\n",
    "\n",
    "df[df[col] > 0.5] # Rows where the col column is greater than 0.5\n",
    "df[(df[col] > 0.5) & (df[col] < 0.7)] # Rows where 0.5 < col < 0.7\n",
    "df.sort_values(col1) # Sorts values by col1 in ascending order\n",
    "df.sort_values(col2,ascending=False) # Sorts values by col2 in descending order\n",
    "df.sort_values([col1,col2], ascending=[True,False]) # Sorts values by col1 in ascending order then col2 in descending order\n",
    "df.groupby(col) # Returns a groupby object for values from one column\n",
    "df.groupby([col1,col2]) # Returns a groupby object values from multiple columns\n",
    "df.groupby(col1)[col2].mean() # Returns the mean of the values in col2, grouped by the values in col1 (mean can be replaced with almost any function from the statistics section)\n",
    "df.pivot_table(index=col1, values= col2,col3], aggfunc=mean) # Creates a pivot table that groups by col1 and calculates the mean of col2 and col3\n",
    "df.groupby(col1).agg(np.mean) # Finds the average across all columns for every unique column 1 group\n",
    "df.apply(np.mean) # Applies a function across each column\n",
    "df.apply(np.max, axis=1) # Applies a function across each row\n",
    "\n",
    "#Joining and Combining\n",
    "\n",
    "df1.append(df2) # Adds the rows in df1 to the end of df2 (columns should be identical)\n",
    "pd.concat([df1, df2],axis=1) # Adds the columns in df1 to the end of df2 (rows should be identical)\n",
    "df1.join(df2,on=col1,how='inner') # SQL-style joins the columns in df1 with the columns on df2 where the rows for col have identical values. how can be one of 'left', 'right', 'outer', 'inner'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Model Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Inferencing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Data storytelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources: \n",
    "\n",
    "https://elitedatascience.com/python-cheat-sheet "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
